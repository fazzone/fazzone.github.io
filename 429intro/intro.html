<!DOCTYPE html>
<html>  
  <head>
    <meta charset='utf-8' />
    <link rel="stylesheet" type="text/css" href="../stylesheets/stylesheet.css">
    <link rel="stylesheet" type="text/css" href="introstyle.css">
	<script type="text/x-mathjax-config">
	  MathJax.Hub.Config({
      	tex2jax: {inlineMath: [["$","$"],["\\(","\\)"]]}
	  });
	</script>
	<script type="text/javascript" src="http://cdn.mathjax.org/mathjax/2.1-latest/MathJax.js?config=TeX-AMS_HTML-full"></script>
	<script type="text/javascript" src="diagrams.js"></script>
    <title>Systems Intro</title>
  </head>
  <body onload="processDOM(document.body)">
    <div id="header_wrap" class="outer">
      <header class="inner">
        <a id="forkme_banner" href="https://github.com/fazzone/fazzone.github.com">View on GitHub</a>
        <h1 id="project_title">Introduction to "Computing Systems"</h1>
        <h4 id="project_tagline">"I am sorry to have written so long an introduction; I did not have time to write a short one"</h4>		
      </header>
    </div>
	
    <div id="main_content_wrap" class="outer">
      <section id="main_content" class="inner">
		<div id="contents" style="background-color: rgb(240,240,240);">
		  <h4>contents</h4>
		  (items that aren't links haven't been written yet)
		  <br>
		  (items with * next to them are actually interesting)
		  <ol>
			<li>
			  <a href="#overview">overview</a>
			  <ol>
				<li><a href="#meta">meta</a>
				  <ol>
					<li><a href="#whatisthis">what is this document?</a></li>
					<li><a href="#why">why did you write this?</a></li>
					<li><a href="#feedback">shameless solicitation for feedback</a></li>
				  </ol>
				</li>
				<li><a href="#preface">preface</a>
				  <ol>
					<li><a href="#computingsystems">what is meant by "Computing Systems"?</a></li>
					<li><a href="#whyinteresting">why is this interesting?</a></li>
					<li><a href="#myapproach">my approach</a></li>
				  </ol>
				</li>
			  </ol>
			</li>
			
			<li>
			  <a href="#data"><strong>Part I</strong> - Data & data storage</a>
			  <ol>
				<li><a href="#datavsinformation">* Data vs. Information</a></li>
				<li><a href="#datastorage">How computers store data</a></li>
				<li><a href="#integers">Representing integers:</a>
				  <ol>
					<li><a href="#bcd">Binary Coded Decimal</a></li>
					<li><a href="#onescomptwoscomp">One's Complement & Two's Complement</a></li>
				  </ol>
				</li>
				<li><a href="#text">Representing text:</a>
				  <ol>
					<li><a href="#ebcdic">EBCDIC</a></li>
					<li><a href="#ascii">ASCII</a></li>
					<li><a href="#unicode">Unicode</a></li>
				  </ol>
				</li>
				<li>Representing programs:
				  <ol>
					<li>Machine code</li>
					<li>Introduction to assembly language</li>
				  </ol>
				</li>
			  </ol>
			</li>
			<li>
			  <strong>Part II</strong> - Code & program execution
			  <ol>
				<li>CPUs & ISAs</li>
				<li>Accessing data:
				  <ol>
					<li>Registers</li>
					<li>Memory buses & addressing</li>
					<li>Harvard & von Neumann architectures</li>
					<li>Address modes</li>
				  </ol>
				</li>
				<li>Operating on data:
				  <ol>
					<li>Arithmetic</li>
					<li>Logical operations</li>
				  </ol>
				<li>Control:
				  <ol>
					<li>Jumps & Conditional jumps</li>
					<li>Loops in assembly</li>
					<li>Jump tables</li>
					<li>Procedures & the stack</li>
					<li>Interrupts</li>
				  </ol>
				</li>				
			  </ol>
			</li>

			<li><strong>Part III</strong> - Putting together the system
			  <ol>
				<li>OS Concepts
				  <ol>
					<li>Memory management & virtual memory</li>
					<li>Multitasking</li>
					<li>The system ABI/API</li>
					<li>The compiler & the linker</li>
				  </ol>
				</li>
				<li>Introduction to C (for Java programmers)
				  <ol>
					<li><code>#include</code> and the preprocessor</li>
					<li>Pointers</li>					
					<li><code>struct</code> & <code>union</code></li>
					<li>Dynamic memory allocation: <code>malloc</code> & <code>free</code></li>
				  </ol>
				</li>
				<li>Counterpoint: Introduction to Lisp
				  <ol>
					<li>Symbols & S-expressions</li>
					<li>Cons cells</li>
					<li><code>eval</code></li>
					<li>Memory management in Lisp</li>
					<li>Bootstraping</li>
				  </ol>
				</li>
			  </ol>
			</li>

			<li><strong>Part IV</strong> - Making it Fast
			  <ol>
				<li>Caching</li>
				<li>Branch prediction</li>
				<li>Pipelining, superscalar architectures, and VLIW systems</li>
				<li>RISC vs CISC</li>
			  </ol>
			</li>
			
			
		  </ol>
		</div>
		
		<div id="overview">
		  <h2>overview - an introduction to the introduction</h2>
		  <div id="meta">
			<h3>meta</h3>
			<div id="whatisthis">
			  <h4>what is this document?</h4>
			  <p>This is intended to be a brief introduction to some lower-level problems & solutions in "computing systems" (more on this phrase and why it is
				used later)</p>
			  
			  <p>I assume a reasonable background in programming and general computer knowledge.</p>
			  
			  <p>You might find this page especially useful if you are (like me) a freshman Turing Scholar on winter break from the University of Texas at Austin, 
				who has completed CS 314H (Algorithms & Data Structures) and will take CS 429H (Computer Organization & Architecture) in the spring.</p>
			</div>
			<div id="why">
			  <h4>why did you write this?</h4>
			  <p>This is really a two-part question: 'Why were you motivated to write this?' and 'What qualifies you to write this?'.  The first one is easy:
				Break is really boring.  The second one is more difficult.  The most correct answer is probably 'nothing', but I think there are two reasons
				why you might want to read this as opposed to just breaking out the textbook.</p>
			  <ul>
				<li><strong>Specialization</strong> - I am writing with a very specific audience in mind, so I can reference and make assumptions about what you know.</li>
				<li><strong>Format</strong> - I am trying to be much more casual than a textbook, and much more brief.  In many places, I will probably trade absolute
				  completeness for brevity, though I will try not to sacrifice correctness.</li>
			  </ul>
			  <p>To sum up, the answer to the question "Why would I read this instead of a textbook?" is "Because this document has different design goals that I believe 
				make it more accessible and quicker to comprehend".</p>
			</div>
			<div id="feedback">
			  <h4>Shameless solicitation for feedback</h4>
			  <p>This is (supposed to be) a living document.  Please submit feedback!  If my writing is unclear, if I spell something wrong, if I'm wrong
				about something, or <i>especially</i> if you have an unanswered question - please let me know!  This page lives on GitHub, so you can submit a pull request or
				<a href="https://github.com/fazzone/fazzone.github.com/issues">open an issue</a>.  Or, you can write me directly; me email address is rmcq@utexas.edu</p>
			</div>
		  </div>
		  <div id="preface">
			<h3>preface</h3>
			<div id="computingsystems">
			  <h4>what is meant by "Computing Systems"?</h4>
			  <p>
				A course on "Systems" is generally considered part of the core of an education in Computer Science, which is most generally defined as the study of
				computation.  In order to give that definition substance, we also have to define computation, which is rather difficult.  We humans have been computing
				since we became a species; our ancestors computed the optimum trajectory of their spears so that they could bring down a mammoth and eat, the best path
				to run to avoid being eaten by a tiger, and so on.  At some point, we invented numbers and other notation to make computation easier.  Now you could
				precisely compute the exact number of sheep you needed to shear to have enough wool to sell to buy food over the winter.  Since
				then, we've advanced slightly as a species, and our computing needs advanced with us.  Now we use machines to compute all sorts of things: the lowest 
				grade you can get on your final and still pass, the cheapest way to fly from Austin to New York, how to win a game of chess, and just about everything
				else.  Thus, the best definition I can think of for computation is 'transforming data in some meaningful way'.  You might also define a computation as:
				'a process which depends on some amount of information as input, and produces some amount of information as output'.  Of course, these definitions
				(just like our first) reference other terms; we will discuss technical definitions of the terms referenced in later sections (probably).
			  </p>
			  <p>The generality of my chosen definition of computation is intentional - but if we've been computing since the dawn of our species, why is the field of 
				computer science so young?  Digital computers and other fancy machines certainly make certain kinds of computation easier, but you certainly can study
				computation without them.  So why did computer science only really get established as a discipline until the invention of the computer?  To be honest,
				I don't have an answer.  The best I can come up with is that before we had machines to compute for us, the only way to compute something was to learn
				the necessary math (for computing is deeply entwined with mathematics) and work it out yourself.  The bottleneck of this process was comprehending the
				math - As long as you understood, mathematically, what you needed to compute, the computation itself was generally little more than an afterthought by
				comparison.  Only when we had to explain the process of computation to a machine, unable to 'understand' the math as we do, did we really think to 
				separate <i>what</i> was being computed with <i>how</i> it was computed.  And by that circuitous route we arrive at my definition of computer science:
				"the study of <i>how</i>"
			  <p>In accordance with its origins, the field of computer science is most often studied with digital computers - a computer program is essentially a 
				distillation of mathematical understanding into a form that can be fed to a "computing system".  This implies my definition of "Computing System":
				something that executes a program.  The computing systems that we are most interested in are 'stored-program digital computers' - this term reflects
				the fact that not all computers necessarily "store" their programs in a way that can be easily changed (it might be baked into the hardware), and that
				not all computers have to be digital (operating by electrical impulses that considered to be in a discrete set of states (commonly 1 and 0) - a compute
				might also be electromechanical (like <a href="https://en.wikipedia.org/wiki/Z3_(computer)">the Z-3</a>) or just mechanical (like Babbage's
				<a href="https://en.wikipedia.org/wiki/Analytical_engine">Analytical Engine</a>)).  The study of 'Systems', then, for our purposes, is the study of the
				properties, design, and implementation of stored-program digital computers.  
			  <p>
			</div>
			<div id="whyinteresting">
			  <h4>why is this interesting?</h4>
			  <p>A "Systems" class is a core part of pretty much every computer science degree because it gives you grounding in what is actually happening when you
				program.  I think this is interesting in and of itself, but it is also helpful when designing for performance - understanding what is going on at
				multiple levels of the system makes it easier to write performant programs.  Also, it is important to note that no programs execute in a vacuum - by
				definition, a program executes on a computer system, so it is good to have knowledge of the system(s) your program run(s) on.  A course in systems, 
				however, is about more than that.  As a programmer, you interact with sytems.  The point of education is to be a critical observer of the world around you
				- in order to be a critical observer of something, you must understand the <i>choices</i> in its design.  A student of political science analyzes 
				governments and can tell you why certain governments are they way they are (and the implications of this),  a student of literature analyzes different
				works and can tell you why certain works are written the way they are (and the implications of this), and a student of computer science should be able
				to do the same with programs.  Since a system defines the behavior of its programs, understanding systems is a necessary skill in computer science.				
			  </p>
			  <p>Furthermore, a good computer scientist should be educated in systems so as to be prepared for things going wrong 
				(and <a href="http://www.joelonsoftware.com/articles/LeakyAbstractions.html">leaky abstractions</a>).  Abstraction is the key way we deal with complexity
				in computer science (and any field, really).  The goal of an abstraction is to 'seal off' complex implementation details and present a simple model for 
				understanding something.  "Pushing the gas makes the car go" is a good abstraction for your car's drivetrain - right up until something goes wrong.  Then
				all of a sudden your abstraction is broken and you have no idea why.  Similarly, you can just assume the entire stack of technology going from solid-state
				physics to the standard library of whatever programming language you are using "just works", or you can be prepared for things going wrong.  Most of the
				things in that open interval would be considered "systems".
			</div>
			<div id="myapproach">
			  <h4>my approach</h4>
			  <p>I couldn't help but skirt around it in previous sections, but my approach to talking about things is going to be to present problems & solutions.
				I think this is the cleanest way to quickly gain a deep understanding of all sorts of things that are designed by humans - all design efforts have
				problems, trade-offs, choices, pressures, goals, etc.  If you understand the problems, you can put yourself in the designers' shoes and think how
				you would come up with a solution - and then compare your way to the 'actual' way.  This approach is also easy to explain in chronological fashion -
				the first attempts at a design are often the simplest, and only after they are used and studied are their shortcomings realized and fixed in later
				designs.  In all cases where it is practical, I will try to present as many alternatives as possible to each design problem.  An easy way to check
				your understanding of what I'm talking about (or things in general) is to try to imagine the consequences if different choices were made in the design
				of some system.  Some examples: What would our government be like if we had <a href="https://en.wikipedia.org/wiki/Proportional_representation">
				  proportional representation</a> (as opposed to winner-takes-all elections) in the House & Senate?  What would our language be like if we used an 
				ideographic writing system (as does Chinese)?  These questions become more interesting the more fundamental the change: it is not very interesting to
				consider how Java would be different if it used a colon (like C++) instead of the word <code>extends</code> to denote inheritance - it would be more
				interesting to consider
				how Java would be different if it used <a href="https://en.wikipedia.org/wiki/Prototype-based_programming">prototypal inheritance</a> instead of the more 
				conservative class-based model.  I will try to include some such considerations as I discuss computing systems.
			  </p>
			</div>
		  </div>
		</div>
		
		<div id="data">
		  <h2>Data & data storage</h2>
		  <div id="datavsinformation">
			<h3>Data vs. Information</h3>
			<p>Before we get into a discussion of how different kinds of data are represented in different kinds of systems, I think that we should first discuss
			  the difference between data and information.  This might be a pretty fluffy distinction, and the concepts do certainly overlap, but I think that it's
			  worth discussing if only because I think the theory of information is very interesting.  To paraphrase <a href="https://en.wikipedia.org/wiki/Data">
				Wikipedia</a>, the difference between data and information is the level of abstraction.  Data is just, well data; the value of some variable.  
			  Only when a human interprets the data does it become information.  The fundamental unit of both is the bit - a binary distinction of some kind: "yes"
			  or "no", "true" or "false", "1" or "0", etc.  As an example (which I believe I am stealing from
			  <a href="http://www.amazon.com/Decoding-Universe-Information-Explaining-Everything/dp/0143038397"><i>Decoding the Universe</i></a>), consider the
			  lighting of the lanterns in the steeple of the Old North Church during the Revolutionary War - one if by land, two if by sea.  Anyone who saw the
			  steeple could have collected
			  the one bit of data (were there two lanterns or one?), but only the patriots across the river were able to decode this data into the corresponding one
			  bit of <i>information</i>: were the British coming by land or by sea?  This is the distinction between data & information at its most immaterial: here, both
			  are pretty much the same thing.  However, had the patriots already discovered by some other means that the British army was coming by sea, the one bit
			  of data would have revealed to them zero bits of information.  This suggests another distinction between information and data: information, by defintion,
			  is non-redundant.  If I write you a letter, but misspell some words, your ability to understand my meaning in spite of my poor spelling is an example of 
			  redundancy - I used more data than required to transmit the amount of information in the letter. </p>
			<p>The mathematical name for this concept is <a href="https://en.wikipedia.org/wiki/Entropy_(information_theory)">information content</a> which, interestingly, 
			  is closely related to the notion of entropy in thermodynamics.  In information theory, the entropy of some variable is
			  a measure of how unpredictable (<i>non</i>-redundant) it is.  The previous example of the letter is made possible because "English text has about one bit of
			  entropy for each character"<sup><a href="https://en.wikipedia.org/wiki/Entropy_(information_theory)#cite_ref-8">[wiki]</a></sup>.  That statement implies
			  that, given a stream of characters representing English text, you could be expected to accurately guess the next character about half the time.  This is
			  how compression algorithms work - they exploit low-entropy source material and use less redundant forms of encoding to save (data) space.  Given a completely
			  random input, a compression algorithm should be expected to not compress it all - or possibly even make it larger (we see this by the pigeonhole principle; 
			  if a compression algorithm is able to make some inputs smaller, it must make a corresponding number of inputs larger - otherwise you'd be encoding information
			  "for free").  This, in turn, is why compressing something that is already compressed (a JPEG image or an MP3 music file) is stupid.  </p>
			<p>If you want to read more about information theory, the book I cited previously,
			  <a href="http://www.amazon.com/Decoding-Universe-Information-Explaining-Everything/dp/0143038397"><i>Decoding the Universe</i></a> is a great read.  You can
			  also read Claude Shannon's original paper <a href="https://docs.google.com/a/utexas.edu/viewer?a=v&q=cache:UlvwzqigMQIJ:cm.bell-labs.com/cm/ms/what/shannonday/shannon1948.pdf+&hl=en&gl=us&pid=bl&srcid=ADGEEShzLsrhAOZxjkAuOTPSBYERGoupPl-DgoyizF5Oqkxxwp-NXvR7e-Wo_-t5fqmLZZRBu-G4uRpREeOYdvdcLJAj9LYk6HwCYej-aGyiy5elzwPOpsg5SSZgVabP9mcrUus0Z8h1&sig=AHIEtbSz7BkLkwZD4sTDOmRe6GzVJAdAsg">A Mathematical Theory of Communication</a> - it is reasonably accessible, as far as papers which create entire new fields of mathematics go.  (Shannon is also famous for writing what is commonly hailed as the most important master's thesis of all time, in which he realized the connection between Boolean logic and
			  digital circuits - a direct antecedent to the construction of the very first digital computers).
		  </div>
		  
		  <div id="datastorage">
			<h3>How computers store data</h3>
			<p>As I discussed previously, the main focus of this modern computing is on digital stored-program computers.  Digital, in this term, refers to how these
			  computers store data.  In a digital system, we deal with discrete values (as opposed to an analog system, where we would deal with continuous values).
			  In the real world (quantum-mechanical nitpicks aside), variables such as voltage across a wire are continuous.  Thus, in digital circuit design, we 
			  view voltages above some threshold as "high" (1), and those below it as "low" (0).  This binary quantization is completely arbitrary, we might just as 
			  well divide the continuous range of voltages into three discrete parts - then we would have a ternary computer.  In practice, however, every computer
			  system has used (at least at this level) binary.  Ternary computers remain an interesting curiosity, see <a href="http://tunguska.sourceforge.net/about.html">
				Tunguska</a> for a ternary computer emulator.</p>
			<p>Data storage, then, in most computers, is just a sequence of bits.  Mathematically speaking, computers store data as a sequence of <i>symbols</i> drawn
			  from the set of possible symbols, called the <i>alphabet</i> (commonly denoted $\Sigma$).  A binary computer is simply the case where $\Sigma = \{1,0\}$
			  .  Note that "1" and "0" are completely arbitrary here; all N-symbol alphabets are completely isomorphic.  We could just as easily have $\Sigma = 
			  \{ðŸ˜ƒ, ðŸ˜ž\}$ or whatever.  Any data we store is simply a way we have of interpreting a sequence of symbols.  Thus, to say "to a computer
			  everything is a number" is (in my opinion) not really accurate, because even the concept of a number is too high-level.  All that exists is a sequence of high
			  and low voltages, or a sequence of symbols - depending if you are an engineer or a mathematician.  All the computer does is manipulate bits (data), and
			  any meaning (information) we attribute to the process is a result of interpretations that exist solely in our minds (and their extensions, our programs).
			  
			  <div id="integers">
				<h4>Representing integers</h4>
				<p>One of the most fundamental things we might wish to represent with a computer are integers.  Since our computers work with discrete symbols,
				  and the integers are discrete objects, the representation is rather natural, though not without its little quirks.  Contrast this with the
				  the reals, where any attempt at representation is doomed to fail - no matter what alphabet or encoding you use, no fixed-length string can hold
				  enough information to represent any real number.  We get by with floating-point (the common standard is <a href="https://en.wikipedia.org/wiki/IEEE_floating_point">IEEE 754</a>), but I consider specific details of floating-point representation outside the scope of this introduction.  (although you should read 
				  <a href="http://docs.oracle.com/cd/E19957-01/806-3568/ncg_goldberg.html">What Every Computer Scientist Should Know About Floating-Point Arithmetic</a>)</p>
				<div id="bcd">
				  <h5>Binary coded decimal</h5>
				  <p>Probably one of the earliest widespread encodings for integers was binary-coded-decimal, or BCD.  In a BCD representation, each base-10 digit of
					a number is encoded seprately, in binary.  That is, </p>
					<table id="bcd_table" style="margin-left: 40px;" border="1"></table>
				  <p>Seems reasonable enough so far.  But in BCD, <i>each decimal digit is encoded individually</i> (hence the name) - for example, the number <code>129</code>
					might be encoded as <code>0001 0010 1001</code>.  Demo: type in a number, see its BCD encoding. (does not work with negatives; BCD traditionally uses
				  a sign digit at the beginning to denote...well, sign)</p>
				  <div>
					<input type="text" value="129" id="bcddemoinput" oninput = "redrawBCDDemo()" />
					<table id="bcddemotable" style="margin-left: inherit; text-align: center;" border="1">
					  <tr border="0"><td>1</td><td>2</td><td>9</td></tr>
					  <tr><td>0001</td><td>0010</td><td>1001</td></tr>
					</table>
				  </div>
					
				  <p>So why use such a seemingly-stupid and baroque system to represent numbers?  Why not just count in binary?  Well, BCD has two main advantages
				  over regular binary place-value encodings.  First, rounding in BCD works just like rounding in decimal: in a regular binary system, numbers such as 0.2
				  cannot be represented exactly (much as 1/3 cannot be represented exactly in a fixed number of decimal digits).  Second, it is very easy to convert
				  BCD to human-readable form, especially in hardware.  A converter that takes the 4 bits of a BCD digit and drives a 7-segment display is just a
				  truth table, and thus can be implemented in hardware.</p>
				  <center>
					From the <a href="http://macao.communications.museum/eng/Exhibition/SecondFloor/MoreInfo/Displays.html">Macao Communications Museum</a>:
					(refers to the individual bits in the BCD representation of a digit as D,C,B,A)
					<br>
					BCD to 7-segment decoder diagram (omits implementation):
					<br>
					<img src="http://macao.communications.museum/images/exhibits/2_18_6_3_eng.png">
					<br>
					<br>
					BCD to 7-segment decoder truth table:
					<br>
					<img src="http://macao.communications.museum/images/exhibits/2_18_6_2_eng.png">
				  </center>
				  <p>
					This is in contrast to a regular binary place-value system, where much more complex hardware would be required to present numbers in decimal format.
					Also, BCD wastes a lot of space compared to a regular binary system.  For some early computers (especially IBM's), the advantages were judged to 
					outweigh the disadvantages, and of course once you have designed a system to use BCD, it is very difficult to change it.  Most designers, however,
					exhibited slightly more sanity, and now (as then) BCD is mostly a curiosity.
				  </p>
				</div>
				<div id="onescomptwoscomp">
				  <h5>One's Complement & Two's Complement</h5>
				  <p>
					Now we'll talk about some systems of representing integers that actually make sense.  What do to with positive numbers is pretty obvious, just
					use a regular binary system with place-value just like how we write decimal.  That is, the number <code>129</code> would be represented as 
					<code>10000001</code> - 1 in 128's place, and 1 in the 1's place for 128 + 1 = 129.  The trick is in representing negative numbers.  In a
					one's complement system, a negative number is represented as being subtracted from the highest representable number.  However, in a two's
					complement system, negative numbers are "subtracted from" the highest representable number <i>plus one</i> - or $2^n$ (where n is the number
					of bits), hence the name.
				  </p>
				  <p>Some examples will make this clearer.  Suppose you have 8 bits, and you want to represent -17.  In a one's complement system, additive negation
					(subtraction from 0) is the same as a bitwise negation, so you take the representation of +17, 10001, and negate it to get 01110.  Now consider
					what happens when you negate 0.  Mathematically, -0 is 0, but in a one's complement system, -0 is the number with all bits set ($2^n -1$).  Of course, 
					using negative zero instead of positive zero won't affect the outcome of any calculations, but the fact that it exists is one of the disadvantages
					of one's complement systems.</p>
				  <p>Essentially all systems today use two's complement, where the number with all bits set is -1 and not -0.  This shifting by one while representing
					negative numbers is the only difference between the two systems.  Two's complement vs. one's complement is a very low-level decision made in the course
					of designing a system - the major differences are observed in designing hardware to do math with binary numbers (an ALU - arithmetic logic unit).
					The design of & distinctions between these circuits is outside the scope of this document, though if you want to learn more about things like that
					(and pretty much everything else that makes up a computer), I highly reccommend <a href="http://www.amazon.com/Code-Language-Computer-Hardware-Software/dp/0735611319"><i>Code</i> by Charles Petzold</a>.
				</div>				  
			  </div>
			  
			  <div id="text">
				<h4>Representing text</h4>
				<p>In addition to integers, another common basic data type that we often manipulate with computers is text.  Much like integers, text is fundamentally
				  discrete, so we can essentially think of a character encoding as a mapping between characters and numbers.  However, this is a leaky abstraction -
				  characters are not necessarily represented in the same was as integers - and while a system usually has only one encoding for numbers, in the age 
				  of the Internet, it is essential to be able to render text encoded with all sorts of disparate schemes.</p>
				<div id="ebcdic">
				  <h5>EBCDIC</h5>
				  <p>EBCDIC is one of the oldest computer character sets in existence.  The acronym stands for "Extended Binary Coded Decimal Interchange Code", and it was (is)
					primarily used (much like its namesake BCD) on IBM mainframes and suchlike.  However, it has persisted long after most system designers were cured
					of the minor insanity which caused them to design systems based on BCD.  Therefore, you might find yourself using the EBCDIC character set on a system
					that uses regular two's-complement to represent numbers.  This has the interesting consequence that 'a'+1 $\ne$ 'b', for example, and you cannot write
					an <code>isLowerCase</code> function as a simple range check.  Much like BCD, EBCDIC is today mostly a curiosity - a story told to young
					programmers to scare them into using the library versions of functions like <code>isLowerCase</code>, which are at least more likely to run on such
					systems strange enough to use EBCDIC.

				</div>
				<div id="ascii">
				  <h5>ASCII</h5>
				  <p>Originally designed for teleprinters, the American Standard Code for Information Exchange was the first widespread, sane character encoding for
					computers.  ASCII is a seven-bit code (though it is commonly stored in eight-bit bytes), that has unique definitions for all uppercase and lowercase
					characters, control characters, and a familiar set of symbols.  To someone who has worked exclusively with ASCII and its derivatives, these choices
					might seem obvious, but at the time they were not.  Before ASCII, the most common teleprinter code was the <a href="https://en.wikipedia.org/wiki/Baudot_code">Baudot code</a>.  The Baudot system is a five-bit code that makes use of shifts to encode more than $2^5$ characters.  It is also non-contiguous like EBCDIC; when interpreted
					  as binary numbers, 'A'+1 $\ne$ 'B'.  When compared to Baudot, ASCII is much easier to work with on a computer - the fact that you have probably never
					had any huge character encoding headaches when dealing with ASCII text is a testament to its engineering.  </p>
				</div>
				<div id="unicode">
				  <h5>Unicode</h5>
				  <p>ASCII is great if you speak English or another language that uses the Roman alphabet, but if you use the Greek or Cyrillic alphabets, or if (God forbid)
					you use an ideographic writing system, you're SOL.  After the IBM PC was released (and cloned all around the world), this became a problem.  So, clone 
					makers in each country would often use the 'extra bit' that falls out of using 8-bit bytes to encode 7-bit ASCII characters, and invent their own
					proprietary extensions to ASCII.  The version of DOS that shipped with the IBM PC used these extra 128 characters to encode all sorts of things from 
					box-drawing characters (allowing you to draw primitive graphical windows in text mode) to smiley faces and playing-card suits.  In other countries,
					these bytes encoded the alphabet of whatever language was spoken in whatever country the computer was being sold.  It should be apparent that this system
					of many conflicting (even for the same language!) encodings was pretty much a huge mess.  With the internet, it became important to standardize this mess,
					so the ISO standardized a bunch of different widespread encodings into <a href="https://en.wikipedia.org/wiki/ISO_8859">ISO 8859</a>.  This basically
					had the function of keeping these kludgy messes of character encodings in use, and generally just sucked.</p>
				  <p>Even though it sucked horribly, ISO 8859 was the <i>de facto</i> Internet standard encoding for a while.  As long as most computers used Windows,
					most people with alphabetical writing systems weren't being actively excluded from the Internet party.  However, other writing systems, specifically
					Japanese (and to a lesser extent Korean) were pretty much the biggest mess around.  Various different encodings fought for dominance, and a few
					were standardized by Japanese Industrial Standards Committee.  Of these, probably the most widespread is an encoding called Shift-JIS, which is
					rather infamous because it "has the unfortunate property that it often breaks any parser ... that is not specifically designed to handle it."<a href="https://en.wikipedia.org/wiki/Japanese_language_and_computers"><sup>[cite]</sup></a> I'm sure that you don't care about any of this, but the main point of this paragraph was to illustrate just what a complete clusterfuck character encoding(s) can
					devolve into if not handled with extreme care.  In other words, it was (is) a worldwide problem just waiting for a solution...  </p>  
				  <p>
					Enter Unicode!  Unicode is a character encoding system designed to handle every human language - an ambitious task, to be sure.  Technically, 
					what the Unicode standard is a mapping of numbers (<i>code points</i> in Unicode parlance) to characters, divided up into <i>code pages</i>.
					Unicode has everything from Cuneiform to Chinese, and is considered (at least by me) as a major victory of international standardization.
					However, even if everyone agrees to use the Unicode code points, there are still compatability problems, because different implementations
					of Unicode can map code points to bit patterns in different ways.  Java uses a system called UTF-16 internally, whever every character is 
					at least 16 bits (hence the difference between <code>char</code>and <code>byte</code> in Java).  I say "at least", because Unicode has over
					$2^{16}$ code points, and UTF-16 represents those with special escape codes that tell the parser to consider the next byte as part of this
					character.  Contrast this to the obsolete encoding UCS-16, which refused to represent code points beyond $2^{16}$.  There is also an encoding
					called UTF-8, where characters take at least 8 bits (and up to 4).  Every Unicode encoding other than UTF-8 has the rather large disadvantage
					of being completely stupid in pretty much every way.  Because it is an 8-bit format, UTF-8 data looks just like ASCII, so long as you
					don't stray outside of the Latin alphabet. However, unlike ASCII, it is perfectly standardized and can represent every Unicode code point,
					so if your software uses UTF-8, users should be able to input text just as well in Chinese and Cyrillic.</p>
				  <p>Seriously, if there is one thing that you take away from this section on data & data representation, <i>please</i> let it be to use UTF-8
					in any systems you design in later life.  Java made the wrongheaded decision to use UTF-16 for various stupid reasons, so the ship has
					pretty much sailed there, but in every single program I have ever written that has had to deal with multilingual text properly, the 
					<i>only</i> way to get it to work has been to use UTF-8 exclusively for both input and output.  By imploring you to use UTF-8, I wish only 
					to save you, and future users of any systems you may design, from the horrible pain of dealing with any other character encoding than Unicode,
					and any other representation for it	than UTF-8.</p>
				  
				</div>
			  </div>
			  
			  <div id="code">
				<h4>Representing code</h4>
			  </div>
		  </div>
		</div>
		
      </section>
    </div>

    <div id="footer_wrap" class="outer">
      <footer class="inner">
        <p>Published with <a href="http://pages.github.com">GitHub Pages</a></p>
		<p>Math rendered with <a href="http://www.mathjax.org/">MathJax</a></p>
      </footer>
    </div>
  </body>
</html>

